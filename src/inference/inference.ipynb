{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/llama-cpp-python/\n",
    "import llama_cpp\n",
    "llama_cpp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"local_models/ascii-merged.Q4_K_M.gguf\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_PROMPT = \"\"\"\n",
    "Generate ascii art that matches the following description.\n",
    "\n",
    "### description:\n",
    "{description}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ascii_art(description: str) -> str:\n",
    "    prompt = INFERENCE_PROMPT.format(description=description)\n",
    "    resp = llm(prompt, max_tokens=256)\n",
    "    return resp[\"choices\"][0][\"text\"]\n",
    "\n",
    "ascii_art = generate_ascii_art(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ascii visualization:\n",
      "<ascii>\n",
      "        ____\n",
      "       |    |\n",
      "     _/    | |  _-_\n",
      "    |       | | |  |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |        |   |\n",
      "    |\n"
     ]
    }
   ],
   "source": [
    "print(ascii_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier komt rommel uit, denk ik omdat de lora nog eens naar f16 is gegaan\n",
    "\n",
    "def generate_ascii_art_with_lora(description: str) -> str:\n",
    "    prompt = INFERENCE_PROMPT.format(description=description)\n",
    "    resp = llm(prompt, max_tokens=256, lora_path=\"local_models/ascii-art-cats-lora-v1-f16.gguf\")\n",
    "    return resp[\"choices\"][0][\"text\"]\n",
    "\n",
    "ascii_art = generate_ascii_art(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ascii visualization:\n",
      "<ascii>\n",
      "          ___\n",
      "       _.-^`    .\n",
      "    .-'   \\__    \\_.\n",
      "  ,'   __/   \\_   _\\_\n",
      " /    \\   /\\   \\   \\\n",
      "|  |  |  |  \\   \\   |\n",
      "\\  \\  \\  \\  |  |  | |\n",
      " \\  \\  \\ \\ \\    \\  \\  \\\n",
      "  \\  \\  \\_\\ \\    \\  \\  \\\n",
      "   \\  \\    \\ \\   \\  \\  \\_\n",
      "    \\  \\   \\ \\   \\  \\  \\ \\\n",
      "     \\ \\    \\ \\   \\  \\  \\ \\\n",
      "      \\ \\   \\ \\   \\  \\  \\ \\\n",
      "       \\_\\   \\_\\   \\  \\  \\ \\\n",
      "        `-\\___-`   \\  \\  \\ \\\n",
      "                      \\  \\  \\ \\\n",
      "                       \\  \\  \\ \\\n",
      "                        \\  \\  \\ \\\n",
      "                         \\  \\  \\ \\\n",
      "                          \\  \\  \\ \\\n",
      "                           \\  \\  \\ \\\n",
      "                            \\  \\  \\ \\\n",
      "                             \\  \\  \\ \\\n",
      "                              \\  \\  \\ \\\n",
      "                               \\  \\  \\ \\\n",
      "                                \\  \\\n"
     ]
    }
   ],
   "source": [
    "print(ascii_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
