{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama.cpp python inference\n",
    "https://llama-cpp-python.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference over our trained ascii adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local paths to the gguf base model and the lora adapter for generating ascii art\n",
    "# get ascii art lora gguf from https://huggingface.co/pookie3000/Llama-3.2-3B-ascii-cats-lora-GGUF\n",
    "# get llama 3.2 base gguf from https://huggingface.co/pookie3000/Llama-3.2-3B-GGUF\n",
    "# store them locally and point to them here\n",
    "lora_path = \"../../local_models/adapters/gguf/Llama-3.2-3B-ascii-cats-lora.gguf\"\n",
    "base_model_path = \"../../local_models/base_models/gguf/llama32.gguf\"\n",
    "\n",
    "\n",
    "llm = Llama(model_path=base_model_path, lora_path=lora_path, verbose=False, n_ctx=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ascii_art(max_tokens: int, generation_config) -> str:\n",
    "    prompt = \"\"\n",
    "    for chunk in llm.create_completion(\n",
    "        prompt, \n",
    "        max_tokens=max_tokens, \n",
    "        stream=True, \n",
    "        temperature=generation_config[\"temperature\"], \n",
    "        top_p=generation_config[\"top_p\"], \n",
    "        min_p=generation_config[\"min_p\"], \n",
    "        frequency_penalty=generation_config[\"frequency_penalty\"], \n",
    "        presence_penalty=generation_config[\"presence_penalty\"], \n",
    "        repeat_penalty=generation_config[\"repeat_penalty\"], \n",
    "        top_k=generation_config[\"top_k\"]\n",
    "    ):\n",
    "        chunk_text = chunk[\"choices\"][0][\"text\"]\n",
    "        print(chunk_text, end=\"\", flush=True)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nucleus sampling https://arxiv.org/pdf/1904.09751\n",
    "default_generation_config = {\n",
    "    # Higher values are more random. OpenAI recommmends either using temperature or top_p, but not both No effect if temperature is set to 1\n",
    "    \"temperature\" : 1,\n",
    "    # Model only considers the smallest set of most probable tokens whose cumulative probability exceeds top_p. No effect if top_p is set to 1\n",
    "    \"top_p\" : 0.95,\n",
    "    # Minimum probability required to sample a token\n",
    "    \"min_p\" : 0.05,\n",
    "    # Positive values penalize new tokens based on their existing frequency in the text so far\n",
    "    \"frequency_penalty\" : 0.0,\n",
    "    # Positive values penalize new tokens based on whether they appear in the text so far.\n",
    "    \"presence_penalty\" : 0.0,\n",
    "    # The penalty to apply to repeated tokens\n",
    "    \"repeat_penalty\" : 1.0,\n",
    "    # Only consider top_k highest probability tokens for each step\n",
    "    \"top_k\" : 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generation config: \", default_generation_config)\n",
    "\n",
    "for i in range(100):\n",
    "    print(f\"Generating ascii art {i+1} of 50\\n\")\n",
    "    generate_ascii_art(max_tokens=200, generation_config=default_generation_config)\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
