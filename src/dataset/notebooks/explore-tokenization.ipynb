{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some playing around with the llama 3 tokenizer. Checking if tokenizer can handle ascii art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\")\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_lossy(tokenizer, text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    token_ids = tokens[\"input_ids\"][0]\n",
    "    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    return decoded_text != text\n",
    "\n",
    "def check_encode_decode(tokenizer, text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    token_ids = tokens[\"input_ids\"][0]\n",
    "    raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    print(f\"Input text:\")\n",
    "    print(text)\n",
    "    print(f\"Encoded-Decoded:\")\n",
    "    print(decoded_text)\n",
    "    print(f\"Raw tokens:\")\n",
    "    print(raw_tokens)\n",
    "    print(f\"Lossy: {check_if_lossy(tokenizer, text)}\")\n",
    "    print()\n",
    "\n",
    "def encode_decode_check(tokenizer, text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    token_ids = tokens[\"input_ids\"][0]\n",
    "    raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    clean_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    print(token_ids)\n",
    "    print(\"Raw tokens:\", raw_tokens)\n",
    "    print(\"Clean text:\", clean_text)\n",
    "    if clean_text == text:\n",
    "        print(\"Tokenization is lossless\")\n",
    "    else:\n",
    "        print(\"Tokenization is lossy\")\n",
    "        \n",
    "def check_if_dataset_lossy(tokenizer, dataset_path):\n",
    "    paths = sorted(list(dataset_path.glob(\"**/*.txt\")))\n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            text = f.read()\n",
    "            if check_if_lossy(tokenizer, text):\n",
    "                print(f\"Lossy tokenization: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_art = r\"\"\"\n",
    "    /\\_/\\           ___\n",
    "   = o_o =_______    \\ \\ \n",
    "    __^      __(  \\.__) )\n",
    "(@)<_____>__(_____)____/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Llama Tokenizer:\")\n",
    "encode_decode_check(llama_tokenizer, ascii_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"../ascii_art/animals/\")\n",
    "\n",
    "check_if_dataset_lossy(llama_tokenizer, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossy_ascii_example = r\"\"\"\n",
    "       _.---._    /\\\\\n",
    "    ./'       \"--`\\//\n",
    "  ./              o \\\n",
    " /./\\  )______   \\__ \\\n",
    "./  / /\\ \\   | \\ \\  \\ \\\n",
    "   / /  \\ \\  | |\\ \\  \\7\n",
    "    \"     \"    \"  \"       \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_encode_decode(llama_tokenizer, lossy_ascii_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossy_ascii_example_2 = \" .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_encode_decode(llama_tokenizer, lossy_ascii_example_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossy_ascii_example_3 = \" s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_encode_decode(llama_tokenizer, lossy_ascii_example_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding\n",
    "print(llama_tokenizer.decode(llama_tokenizer.convert_tokens_to_ids(['Ġ?'])))  # Output: \".\"\n",
    "print(llama_tokenizer.decode(llama_tokenizer.convert_tokens_to_ids(['Ġa'])))  # Output: \" a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding\n",
    "print(gpt2_tokenizer.decode(gpt2_tokenizer.convert_tokens_to_ids(['Ġ?'])))  # Output: \".\"\n",
    "print(gpt2_tokenizer.decode(gpt2_tokenizer.convert_tokens_to_ids(['Ġa'])))  # Output: \" a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"llama_tokenizer.clean_up_tokenization_spaces: {llama_tokenizer.clean_up_tokenization_spaces}\")\n",
    "print(f\"gpt2_tokenizer.clean_up_tokenization_spaces: {gpt2_tokenizer.clean_up_tokenization_spaces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dataset_path = Path(\"../ascii_art/animals/\")\n",
    "check_if_dataset_lossy(gpt2_tokenizer, dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to disable clean_up_tokenization_spaces to get correct ascii for llama tokenizer.\n",
    "https://huggingface.co/docs/transformers/en/model_doc/llama#transformers.LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer.clean_up_tokenization_spaces = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dataset_path = Path(\"../ascii_art/animals/\")\n",
    "check_if_dataset_lossy(llama_tokenizer, dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if ascii token is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_ASCII_PROMPT = \"\"\"\n",
    "Generate ascii art that matches the following description.\n",
    "\n",
    "### description:\n",
    "{description}\n",
    "\n",
    "### ascii visualization:\n",
    "<ascii>\n",
    "{ascii_art}\n",
    "</ascii>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(tokenizer, text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    token_ids = tokens[\"input_ids\"][0]\n",
    "    raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    print(\"Original text:\")\n",
    "    print(text)\n",
    "    print(\"Tokenized text:\")\n",
    "    print(raw_tokens)\n",
    "\n",
    "tokenize_text(llama_tokenizer, OUTPUT_ASCII_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': ['<ascii>','</ascii>']}\n",
    "num_added_toks = llama_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenize_text(llama_tokenizer, OUTPUT_ASCII_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [128000, 198, 128000, 198, 32215, 48220, 1989, 430, 9248, 279, 2768, 4096, 382, 14711, 4096, 512, 4719, 271, 14711, 48220, 42148, 512]\n",
    "decoded_text =llama_tokenizer.convert_ids_to_tokens(tokens, skip_special_tokens=False)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = 128000\n",
    "decoded_text =llama_tokenizer.convert_ids_to_tokens(tokens, skip_special_tokens=False)\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
