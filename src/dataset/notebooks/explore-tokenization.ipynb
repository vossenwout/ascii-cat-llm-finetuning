{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if tokenizer can handle ascii art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\")\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_lossy(tokenizer, text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    token_ids = tokens[\"input_ids\"][0]\n",
    "    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    return decoded_text != text\n",
    "\n",
    "def check_encode_decode(tokenizer, text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    token_ids = tokens[\"input_ids\"][0]\n",
    "    raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    print(f\"Input text:\")\n",
    "    print(text)\n",
    "    print(f\"Encoded-Decoded:\")\n",
    "    print(decoded_text)\n",
    "    print(f\"Raw tokens:\")\n",
    "    print(raw_tokens)\n",
    "    print(f\"Lossy: {check_if_lossy(tokenizer, text)}\")\n",
    "    print()\n",
    "\n",
    "def encode_decode_check(tokenizer, text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    token_ids = tokens[\"input_ids\"][0]\n",
    "    raw_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    clean_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    print(token_ids)\n",
    "    print(\"Raw tokens:\", raw_tokens)\n",
    "    print(\"Clean text:\", clean_text)\n",
    "    if clean_text == text:\n",
    "        print(\"Tokenization is lossless\")\n",
    "    else:\n",
    "        print(\"Tokenization is lossy\")\n",
    "        \n",
    "def check_if_dataset_lossy(tokenizer, dataset_path):\n",
    "    paths = sorted(list(dataset_path.glob(\"**/*.txt\")))\n",
    "    for path in paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            text = f.read()\n",
    "            if check_if_lossy(tokenizer, text):\n",
    "                print(f\"Lossy tokenization: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_art = r\"\"\"\n",
    "    /\\_/\\           ___\n",
    "   = o_o =_______    \\ \\ \n",
    "    __^      __(  \\.__) )\n",
    "(@)<_____>__(_____)____/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama Tokenizer:\n",
      "tensor([128000,    198,    262,  24445,     62,  35419,   1881,   7588,    198,\n",
      "           256,    284,    297,  14513,    284,   2179,   6101,    262,   1144,\n",
      "          1144,    720,    262,   1328,     61,    415,  15990,    220,   1144,\n",
      "          4952,      8,   1763,   6084,  27530,  81617,     29,   3889,   2179,\n",
      "         16726,   2179,   6018])\n",
      "Raw tokens: ['<|begin_of_text|>', 'Ċ', 'ĠĠĠ', 'Ġ/\\\\', '_', '/\\\\', 'ĠĠĠĠĠĠĠĠĠĠ', 'Ġ___', 'Ċ', 'ĠĠ', 'Ġ=', 'Ġo', '_o', 'Ġ=', '____', '___', 'ĠĠĠ', 'Ġ\\\\', 'Ġ\\\\', 'ĠĊ', 'ĠĠĠ', 'Ġ__', '^', 'ĠĠĠĠĠ', 'Ġ__(', 'Ġ', 'Ġ\\\\', '.__', ')', 'Ġ)Ċ', '(@', ')<', '_____', '>', '__(', '____', '_)', '____', '/Ċ']\n",
      "Clean text: \n",
      "    /\\_/\\           ___\n",
      "   = o_o =_______    \\ \\ \n",
      "    __^      __(  \\.__) )\n",
      "(@)<_____>__(_____)____/\n",
      "\n",
      "Tokenization is lossless\n"
     ]
    }
   ],
   "source": [
    "print(\"Llama Tokenizer:\")\n",
    "encode_decode_check(llama_tokenizer, ascii_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lossy tokenization: ../ascii_art/animals/aardvark/0/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/aardvark/1/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/bat/1/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/bear/0/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/beaver/0/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/beaver/1/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/beaver/2/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/beaver/3/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/birds/10/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/birds/2/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/birds/4/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/birds/6/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/birds/8/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/birds/9/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/bison/0/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/camel/0/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/camel/1/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/camel/2/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/camel/4/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/10/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/100/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/101/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/103/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/104/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/105/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/106/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/108/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/109/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/111/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/112/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/114/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/116/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/117/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/118/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/119/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/14/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/15/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/16/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/21/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/22/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/23/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/25/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/27/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/28/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/29/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/3/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/30/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/31/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/32/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/33/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/36/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/39/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/40/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/41/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/45/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/46/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/47/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/48/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/50/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/52/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/57/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/58/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/59/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/6/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/62/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/63/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/64/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/65/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/66/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/67/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/68/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/69/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/7/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/71/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/72/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/73/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/74/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/75/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/76/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/77/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/79/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/8/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/80/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/81/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/82/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/85/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/86/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/88/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/89/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/90/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/91/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/92/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/94/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/96/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/98/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cat/99/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cow/2/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/cow/3/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/deer/0/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/deer/3/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/dog/10/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/dog/11/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/dog/12/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/dog/13/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/dog/6/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/dog/7/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/dog/9/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/dolphin/0/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/dolphin/1/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/dolphin/2/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/elephant/0/content.txt\n",
      "Lossy tokenization: ../ascii_art/animals/elephant/1/content.txt\n"
     ]
    }
   ],
   "source": [
    "dataset_path = Path(\"../ascii_art/animals/\")\n",
    "\n",
    "check_if_dataset_lossy(llama_tokenizer, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossy_ascii_example = r\"\"\"\n",
    "       _.---._    /\\\\\n",
    "    ./'       \"--`\\//\n",
    "  ./              o \\\n",
    " /./\\  )______   \\__ \\\n",
    "./  / /\\ \\   | \\ \\  \\ \\\n",
    "   / /  \\ \\  | |\\ \\  \\7\n",
    "    \"     \"    \"  \"       \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "\n",
      "       _.---._    /\\\\\n",
      "    ./'       \"--`\\//\n",
      "  ./              o \\\n",
      " /./\\  )______   \\__ \\\n",
      "./  / /\\ \\   | \\ \\  \\ \\\n",
      "   / /  \\ \\  | |\\ \\  \\7\n",
      "    \"     \"    \"  \"       \n",
      "\n",
      "Encoded-Decoded:\n",
      "\n",
      "       _.---._    /\\\\\n",
      "   ./'       \"--`\\//\n",
      " ./              o \\\n",
      " /./\\  )______   \\__ \\\n",
      "./  / /\\ \\   | \\ \\  \\ \\\n",
      "   / /  \\ \\  | |\\ \\  \\7\n",
      "    \"     \"    \"  \"       \n",
      "\n",
      "Raw tokens:\n",
      "['<|begin_of_text|>', 'Ċ', 'ĠĠĠĠĠĠ', 'Ġ_.', '---', '._', 'ĠĠĠ', 'Ġ/', '\\\\\\\\', 'Ċ', 'ĠĠĠ', 'Ġ.', \"/'\", 'ĠĠĠĠĠĠ', 'Ġ\"--', '`\\\\', '//Ċ', 'Ġ', 'Ġ./', 'ĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġo', 'Ġ\\\\Ċ', 'Ġ/', './', '\\\\', 'Ġ', 'Ġ)', '____', '__', 'ĠĠ', 'Ġ\\\\', '__', 'Ġ\\\\Ċ', './', 'Ġ', 'Ġ/', 'Ġ/\\\\', 'Ġ\\\\', 'ĠĠ', 'Ġ|', 'Ġ\\\\', 'Ġ\\\\', 'Ġ', 'Ġ\\\\', 'Ġ\\\\Ċ', 'ĠĠ', 'Ġ/', 'Ġ/', 'Ġ', 'Ġ\\\\', 'Ġ\\\\', 'Ġ', 'Ġ|', 'Ġ|\\\\', 'Ġ\\\\', 'Ġ', 'Ġ\\\\', '7', 'Ċ', 'ĠĠĠ', 'Ġ\"', 'ĠĠĠĠ', 'Ġ\"', 'ĠĠĠ', 'Ġ\"', 'Ġ', 'Ġ\"', 'ĠĠĠĠĠĠĠĊ']\n",
      "Lossy: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_encode_decode(llama_tokenizer, lossy_ascii_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossy_ascii_example_2 = \" .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      " .\n",
      "Encoded-Decoded:\n",
      ".\n",
      "Raw tokens:\n",
      "['<|begin_of_text|>', 'Ġ.']\n",
      "Lossy: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_encode_decode(llama_tokenizer, lossy_ascii_example_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossy_ascii_example_3 = \" s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      " s\n",
      "Encoded-Decoded:\n",
      " s\n",
      "Raw tokens:\n",
      "['<|begin_of_text|>', 'Ġs']\n",
      "Lossy: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_encode_decode(llama_tokenizer, lossy_ascii_example_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      " a\n"
     ]
    }
   ],
   "source": [
    "# Decoding\n",
    "print(llama_tokenizer.decode(llama_tokenizer.convert_tokens_to_ids(['Ġ?'])))  # Output: \".\"\n",
    "print(llama_tokenizer.decode(llama_tokenizer.convert_tokens_to_ids(['Ġa'])))  # Output: \" a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ?\n",
      " a\n"
     ]
    }
   ],
   "source": [
    "# Decoding\n",
    "print(gpt2_tokenizer.decode(gpt2_tokenizer.convert_tokens_to_ids(['Ġ?'])))  # Output: \".\"\n",
    "print(gpt2_tokenizer.decode(gpt2_tokenizer.convert_tokens_to_ids(['Ġa'])))  # Output: \" a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_tokenizer.clean_up_tokenization_spaces: True\n",
      "gpt2_tokenizer.clean_up_tokenization_spaces: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"llama_tokenizer.clean_up_tokenization_spaces: {llama_tokenizer.clean_up_tokenization_spaces}\")\n",
    "print(f\"gpt2_tokenizer.clean_up_tokenization_spaces: {gpt2_tokenizer.clean_up_tokenization_spaces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dataset_path = Path(\"../ascii_art/animals/\")\n",
    "check_if_dataset_lossy(gpt2_tokenizer, dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to disable clean_up_tokenization_spaces to get correct ascii for llama tokenizer.\n",
    "https://huggingface.co/docs/transformers/en/model_doc/llama#transformers.LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer.clean_up_tokenization_spaces = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dataset_path = Path(\"../ascii_art/animals/\")\n",
    "check_if_dataset_lossy(llama_tokenizer, dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
